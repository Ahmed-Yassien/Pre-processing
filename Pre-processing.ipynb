{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know the type of each column\n",
    "df.dtypes\n",
    "# reminder : our projects can't use string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "# what to look for :   Highest and lowest values to notice the ootliers\n",
    "# If any odd values were found, determine all uniqe values and replace the odd ones\n",
    "apgar_5.unique()\n",
    "apgar_5 = apgar_5.replace([10.5],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([df.combo].mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better yet is using sweetviz or pandas_profiling\n",
    "# pandas_profiling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "\n",
    "#data = pd.read_excel (r'neonatal/neonatal outcomes.xlsx')\n",
    "df = pd.read_excel('neonatal/neonatal outcomes.xlsx', engine='openpyxl')\n",
    "\n",
    "\n",
    "profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
    "\n",
    "profile.to_file(\"output.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweetviz\n",
    "import sweetviz as sv\n",
    "#analyzing the dataset\n",
    "car_report = sv.analyze(df)\n",
    "car_report.show_html('Report.html')\n",
    "#Analyzing a single dataframe (and its -optional- target feature)\n",
    "my_report = sv.analyze([train, \"Train\"],target_feat='SalePrice')\n",
    "my_report.show_html('Report.html')\n",
    "#Comparing two dataframes (e.g. Test vs Training sets)\n",
    "my_report1 = sv.compare([train, \"Train\"], [test, \"Test\"], \"SalePrice\")\n",
    "my_report1.show_html('Report.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the html files in python\n",
    "import IPython\n",
    "IPython.display.HTML('Report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEROID(S) One hot encoding\n",
    "steroids_dum=pd.get_dummies(df.steroids)\n",
    "steroids_dum.columns = ['steroids1', 'steroids2', 'steroids3']\n",
    "df_final = pd.concat([df,steroids_dum],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt # NOTE: This was tested with matplotlib v. 2.1.0\n",
    " \n",
    "#########################\n",
    "#\n",
    "# Data Generation Code\n",
    "#\n",
    "#########################\n",
    "## In this example, the data is in a data frame called data.\n",
    "## Columns are individual samples (i.e. cells)\n",
    "## Rows are measurements taken for all the samples (i.e. genes)\n",
    "## Just for the sake of the example, we'll use made up data...\n",
    "genes = ['gene' + str(i) for i in range(1,101)]\n",
    " \n",
    "wt = ['wt' + str(i) for i in range(1,6)]\n",
    "ko = ['ko' + str(i) for i in range(1,6)]\n",
    " \n",
    "data = pd.DataFrame(columns=[*wt, *ko], index=genes)\n",
    " \n",
    "for gene in data.index:\n",
    "    data.loc[gene,'wt1':'wt5'] = np.random.poisson(lam=rd.randrange(10,1000), size=5)\n",
    "    data.loc[gene,'ko1':'ko5'] = np.random.poisson(lam=rd.randrange(10,1000), size=5)\n",
    " \n",
    "print(data.head())\n",
    "print(data.shape)\n",
    " \n",
    "#########################\n",
    "#\n",
    "# Perform PCA on the data\n",
    "#\n",
    "#########################\n",
    "# First center and scale the data\n",
    "scaled_data = preprocessing.scale(data.T)\n",
    " \n",
    "pca = PCA() # create a PCA object\n",
    "pca.fit(scaled_data) # do the math\n",
    "pca_data = pca.transform(scaled_data) # get PCA coordinates for scaled_data\n",
    " \n",
    "#########################\n",
    "#\n",
    "# Draw a scree plot and a PCA plot\n",
    "#\n",
    "#########################\n",
    " \n",
    "#The following code constructs the Scree plot\n",
    "per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    " \n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    " \n",
    "#the following code makes a fancy looking plot using PC1 and PC2\n",
    "pca_df = pd.DataFrame(pca_data, index=[*wt, *ko], columns=labels)\n",
    " \n",
    "plt.scatter(pca_df.PC1, pca_df.PC2)\n",
    "plt.title('My PCA Graph')\n",
    "plt.xlabel('PC1 - {0}%'.format(per_var[0]))\n",
    "plt.ylabel('PC2 - {0}%'.format(per_var[1]))\n",
    " \n",
    "for sample in pca_df.index:\n",
    "    plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))\n",
    " \n",
    "plt.show()\n",
    " \n",
    "#########################\n",
    "#\n",
    "# Determine which genes had the biggest influence on PC1\n",
    "#\n",
    "#########################\n",
    " \n",
    "## get the name of the top 10 measurements (genes) that contribute\n",
    "## most to pc1.\n",
    "## first, get the loading scores\n",
    "loading_scores = pd.Series(pca.components_[0], index=genes)\n",
    "## now sort the loading scores based on their magnitude\n",
    "sorted_loading_scores = loading_scores.abs().sort_values(ascending=False)\n",
    " \n",
    "# get the names of the top 10 genes\n",
    "top_10_genes = sorted_loading_scores[0:10].index.values\n",
    " \n",
    "## print the gene names and their scores (and +/- sign)\n",
    "print(loading_scores[top_10_genes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers (best with continous data)\n",
    "max_thresold = df['height'].quantile(0.95)\n",
    "min_thresold = df['height'].quantile(0.05)\n",
    "df = df[(df['height']<max_thresold) & (df['height']>min_thresold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word to num\n",
    "from word2number import w2n\n",
    "df.x = x2n.word_to_num(df.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "\n",
    "# plot missing features\n",
    "from feature_selector import FeatureSelector\n",
    "fs = FeatureSelector(data = X, labels = train_labels)\n",
    "fs.identify_missing(missing_threshold=0.6)\n",
    "missing_features = fs.ops['missing']\n",
    "missing_features[:10]\n",
    "fs.plot_missing()\n",
    "\n",
    "\n",
    "# Analyze how many null values are in the dataset\n",
    "# import math\n",
    "#data.isnull().sum()\n",
    "# data.plot_missing()\n",
    "\n",
    "# drop all missing values in all columns:\n",
    "#data = data.dropna()\n",
    "\n",
    "# Mean value imputation:\n",
    "# data[\"Age\"].fillna(data[\"Age\"].mean(), inplace=True)\n",
    "\n",
    "# Check the database after missed data were dropped\n",
    "#data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implanced data\n",
    "ones = data[data['any_outcome']==1]\n",
    "zeros = data[data['any_outcome']==0]\n",
    "print(ones.shape,zeros.shape)\n",
    "\n",
    "# لو طلع في فرق بين الرقمين\n",
    "\n",
    "# Implementing Undersampling بياخد عينة عشوائية من المجموعة الكبيرة\n",
    "nm = NearMiss(random_state=42)\n",
    "X_res,y_res=nm.fit_sample(X,Y)\n",
    "\n",
    "# Implementing Oversampling (بيضاعف المجموعة الصغيرة شبه البوتستراب)\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss\n",
    "smk = SMOTETomek(random_state=42)\n",
    "X_res,y_res=smk.fit_sample(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Composite Outcome\n",
    "C_outcome = df['neonatal_morbidity'] + df ['perinatal_death'] + df['nicu_admission'] + df['resp_support'] + df['apgar_5']\n",
    "C_outcome = C_outcome.replace([2,3,4,5],1)\n",
    "C_outcome.columns = ['C_outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some algorithms work better with normalization (better with continous data)\n",
    "Features =  np.asarray(df_final[['age', 'parity', 'bmi']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name every outcome differently"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
